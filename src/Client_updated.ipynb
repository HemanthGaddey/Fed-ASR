{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "251eaf41-cd08-437c-b01f-3068bedf8e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import grpc\n",
    "import proto.cbsp_pb2 as cbsp_pb2\n",
    "import proto.cbsp_pb2_grpc as cbsp_pb2_grpc\n",
    "from concurrent import futures\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "287ead7e-63e3-44be-8d84-878a6218ed10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClientMessage():\n",
    "    def __init__(self, CbspMsg):\n",
    "        self.CbspMsg = CbspMsg\n",
    "    \n",
    "    # Note: All the serialization and deserialization use Lazy Dict serialization!\n",
    "    # Utility Functions\n",
    "    def torchModel2NumpyParams(self, model): \n",
    "        params = [val.cpu().numpy() for _, val in model.state_dict().items()]\n",
    "        return params\n",
    "\n",
    "    def numpyParams2TorchModel(self, model, params): \n",
    "        model_params = model.state_dict()\n",
    "        for key, val in model_params.items():\n",
    "            if len(params) > 0:\n",
    "                model_params[key] = torch.tensor(params.pop(0))\n",
    "        model.load_state_dict(model_params)\n",
    "        return model\n",
    "                        \n",
    "    # Orphaned Function\n",
    "    def serializeDictStrict(self, info): \n",
    "        info_ = {}\n",
    "        for key, value in info.items():\n",
    "            assert (isinstance(key, str)), \"Dictionary Keys contain non String values!\"\n",
    "            assert (isinstance(value, int) or isinstance(value, str) or isinstance(value, float) or isinstance(value, bool)), \"Dictionary Contains Unsupported Value types!\"\n",
    "            \n",
    "            value_ = None\n",
    "            if(isinstance(value, bool)):    \n",
    "                value_ = self.CbspMsg.Constant(bool=value)\n",
    "            elif(isinstance(value, int)):    \n",
    "                value_ = self.CbspMsg.Constant(sint64=value)\n",
    "            elif(isinstance(value, float)):    \n",
    "                value_ = self.CbspMsg.Constant(double=value)\n",
    "            elif(isinstance(value, str)):    \n",
    "                value_ = self.CbspMsg.Constant(string=value)\n",
    "            else:\n",
    "                print(\"UNKNOWN ERROR CONVERTING DICT TO GRPC MSG FORMAT :( (chusko)\")\n",
    "                \n",
    "            info_[key] = value_\n",
    "        return info_\n",
    "    \n",
    "    def serializeDictLazy(self, info):\n",
    "        return {\"dict\":self.CbspMsg.Constant(string=str(info))}\n",
    "        \n",
    "    def deserializeDictLazy(self, info):\n",
    "        return eval(info['dict'].string)\n",
    "\n",
    "    # Msg Serialization Functions\n",
    "    def serializePytorchParams(self, params): \n",
    "        params_bytelist = []\n",
    "        for i in params:\n",
    "            params_bytelist.append(cbsp_pb2.ParameterBytes(tensor=pickle.dumps(i), shape=[0]))\n",
    "        return cbsp_pb2.PytorchParameters(parameters=params_bytelist, dtype=str(params[0].dtype))\n",
    "                      \n",
    "    def serializeGetParametersMsg(self, info):\n",
    "        info_=self.serializeDictLazy(info)\n",
    "        return self.CbspMsg.ClientMessage(\n",
    "            get_parameters = self.CbspMsg.ClientMessage.GetParameters(\n",
    "                type=self.CbspMsg.ClientMessage.GET_PARAMETERS,\n",
    "                info=info_\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def serializeGetConfigMsg(self, info):\n",
    "        info_=self.serializeDictLazy(info)\n",
    "        return self.CbspMsg.ClientMessage(\n",
    "            get_config = self.CbspMsg.ClientMessage.GetConfig(\n",
    "                type=self.CbspMsg.ClientMessage.GET_CONFIG,\n",
    "                info=info_\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def serializeSendParametersMsg(self, info, model):\n",
    "        info_=self.serializeDictLazy(info)\n",
    "        params = self.torchModel2NumpyParams(model)\n",
    "        params_grpc=self.serializePytorchParams(params) # params_grpc\n",
    "        return self.CbspMsg.ClientMessage(\n",
    "            send_parameters = self.CbspMsg.ClientMessage.SendParameters(\n",
    "                type=self.CbspMsg.ClientMessage.SEND_PARAMETERS,\n",
    "                info=info_,\n",
    "                parameters=params_grpc\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def serializeSendResultsMsg(self, info, results):\n",
    "        info_=self.serializeDictLazy(info)\n",
    "        results_=self.serializeDictLazy(results)\n",
    "        return self.CbspMsg.ClientMessage(\n",
    "            send_results = self.CbspMsg.ClientMessage.SendResults(\n",
    "                type=self.CbspMsg.ClientMessage.SEND_RESULTS,\n",
    "                info=info_,\n",
    "                results=results_\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def numpyParams2TorchModel(self, model, params_grpc, dtype): # TODO: Reduce Redundancy and also naming is dull - bytes vs numpy array\n",
    "        tensor_list = [np.frombuffer(param.tensor, dtype=dtype).reshape(param.shape) for param in params_grpc]\n",
    "        if(model):\n",
    "            model_params = model.state_dict()\n",
    "            for key, val in model_params.items():\n",
    "                if len(tensor_list) > 0:\n",
    "                    model_params[key] = torch.tensor(tensor_list.pop(0))\n",
    "            model.load_state_dict(model_params)\n",
    "        return (None,model)[model != None], tensorl_list, dtype\n",
    "\n",
    "    # Msg Deserialization Functions\n",
    "    def deserializePytorchParams(self, params_grpc): \n",
    "        params_bytelist = params_grpc.parameters\n",
    "        params_dtype = params_grpc.dtype # Redundant\n",
    "        params = []\n",
    "        for i in params_bytelist:\n",
    "            params.append(pickle.loads(i.tensor))\n",
    "        return params\n",
    "        \n",
    "    def deserializeGetParametersMsg(self, request):\n",
    "        info_ = request.get_parameters.info\n",
    "\n",
    "        info = self.deserializeDictLazy(info_)\n",
    "\n",
    "        return info\n",
    "        \n",
    "    def deserializeGetConfigMsg(self, request):\n",
    "        info_ = request.get_config.info\n",
    "\n",
    "        info = self.deserializeDictLazy(info_)\n",
    "\n",
    "        return info\n",
    "        \n",
    "    def deserializeSendParametersMsg(self, request, model=None):\n",
    "        info_ = request.send_parameters.info\n",
    "        params_grpc = request.send_parameters.parameters\n",
    "        \n",
    "        info = self.deserializeDictLazy(info_)\n",
    "        params = self.deserializePytorchParams(params_grpc)\n",
    "\n",
    "        if(model):\n",
    "            model = self.numpyParams2TorchModel(model, params)\n",
    "\n",
    "        return info, params, model # dtype process is sus\n",
    "\n",
    "    def deserializeSendResultsMsg(self, request):\n",
    "        info_ = request.send_results.info\n",
    "        results_ = request.send_results.results\n",
    "\n",
    "        info = self.deserializeDictLazy(info_)\n",
    "        results = self.deserializeDictLazy(results_)\n",
    "\n",
    "        return info, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b2788bc-00a9-403f-90b6-34cb37480bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ServerMessage():\n",
    "    def __init__(self, CbspMsg):\n",
    "        self.CbspMsg = CbspMsg\n",
    "        \n",
    "    # TODO: Reduce redundancy by separating/modularizing utility functions code\n",
    "    # Note: All the serialization and deserialization use Lazy Dict serialization!\n",
    "    # Utility Functions\n",
    "    def torchModel2NumpyParams(self, model): \n",
    "        params = [val.cpu().numpy() for _, val in model.state_dict().items()]\n",
    "        return params\n",
    "\n",
    "    def numpyParams2TorchModel(self, model, params): \n",
    "        model_params = model.state_dict()\n",
    "        for key, val in model_params.items():\n",
    "            if len(params) > 0:\n",
    "                model_params[key] = torch.tensor(params.pop(0))\n",
    "        model.load_state_dict(model_params)\n",
    "        return model\n",
    "        \n",
    "    # Orphaned Function\n",
    "    def serializeDictStrict(self, info): \n",
    "        info_ = {}\n",
    "        for key, value in info.items():\n",
    "            assert (isinstance(key, str)), \"Dictionary Keys contain non String values!\"\n",
    "            assert (isinstance(value, int) or isinstance(value, str) or isinstance(value, float) or isinstance(value, bool)), \"Dictionary Contains Unsupported Value types!\"\n",
    "            \n",
    "            value_ = None\n",
    "            if(isinstance(value, bool)):    \n",
    "                value_ = self.CbspMsg.Constant(bool=value)\n",
    "            elif(isinstance(value, int)):    \n",
    "                value_ = self.CbspMsg.Constant(sint64=value)\n",
    "            elif(isinstance(value, float)):    \n",
    "                value_ = self.CbspMsg.Constant(double=value)\n",
    "            elif(isinstance(value, str)):    \n",
    "                value_ = self.CbspMsg.Constant(string=value)\n",
    "            else:\n",
    "                print(\"UNKNOWN ERROR CONVERTING DICT TO GRPC MSG FORMAT :( (chusko)\")\n",
    "                \n",
    "            info_[key] = value_\n",
    "        return info_\n",
    "    \n",
    "    def serializeDictLazy(self, info):\n",
    "        return {\"dict\":self.CbspMsg.Constant(string=str(info))}\n",
    "        \n",
    "    def deserializeDictLazy(self, info):\n",
    "        return eval(info['dict'].string)\n",
    "\n",
    "    # Msg Serialization Functions        \n",
    "    def serializePytorchParams(self, params): \n",
    "        params_bytelist = []\n",
    "        for i in params:\n",
    "            params_bytelist.append(cbsp_pb2.ParameterBytes(tensor=pickle.dumps(i), shape=[0]))\n",
    "        return cbsp_pb2.PytorchParameters(parameters=params_bytelist, dtype=str(params[0].dtype))\n",
    "   \n",
    "    def serializeSendParametersMsg(self, info, model): # TODO: Get Better Naming for these since pytorch specific\n",
    "        info_=self.serializeDictLazy(info)\n",
    "        params = self.torchModel2NumpyParams(model)\n",
    "        params_grpc=self.serializePytorchParams(params) # params_grpc\n",
    "        return self.CbspMsg.ServerMessage(\n",
    "            get_parameters = self.CbspMsg.ServerMessage.SendParameters( # TODO: Change this get_parameters to send_parameters\n",
    "                type=self.CbspMsg.ServerMessage.SEND_PARAMETERS,\n",
    "                info=info_,\n",
    "                parameters=params_grpc\n",
    "            )\n",
    "        )       \n",
    "\n",
    "    def serializeSendConfigMsg(self, info):\n",
    "        info_=self.serializeDictLazy(info)\n",
    "        return self.CbspMsg.ServerMessage(\n",
    "            send_config = self.CbspMsg.ServerMessage.SendConfig(\n",
    "                type=self.CbspMsg.ServerMessage.SEND_CONFIG,\n",
    "                info=info_\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def serializeNormalResponseMsg(self, info, response):\n",
    "        info_=self.serializeDictLazy(info)\n",
    "        return self.CbspMsg.ServerMessage(\n",
    "            normal_response = self.CbspMsg.ServerMessage.NormalResponse(\n",
    "                type=self.CbspMsg.ServerMessage.MESSAGE_TYPE.NORMAL_RESPONSE,\n",
    "                info=info_,\n",
    "                response=response\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    # Msg Deserialization Functions\n",
    "    def deserializePytorchParams(self, params_grpc): \n",
    "        params_bytelist = params_grpc.parameters\n",
    "        params_dtype = params_grpc.dtype # Redundant\n",
    "        params = []\n",
    "        for i in params_bytelist:\n",
    "            params.append(pickle.loads(i.tensor))\n",
    "        return params\n",
    "        \n",
    "    def deserializeSendParametersMsg(self, request, model=None):\n",
    "        info_ = request.get_parameters.info # TODO: Change this get_parameters to send_parameters\n",
    "        param_bytes = request.get_parameters.parameters # TODO: Change this get_parameters to send_parameters\n",
    "        \n",
    "        info = self.deserializeDictLazy(info_)\n",
    "        params = self.deserializePytorchParams(param_bytes)\n",
    "\n",
    "        if(model):\n",
    "            model = self.numpyParams2TorchModel(model, params)\n",
    "\n",
    "        return info, params, model # dtype process is sus\n",
    "\n",
    "    def deserializeSendConfigMsg(self, request):\n",
    "        info_ = request.send_config.info\n",
    "        info = self.deserializeDictLazy(info_)\n",
    "        return info\n",
    "        \n",
    "    def deserializeNormalResponseMsg(self, request):\n",
    "        info_ = request.normal_response.info\n",
    "        info = self.deserializeDictLazy(info_)\n",
    "        response = request.normal_response.response\n",
    "        return info, response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c382ab46-cf6c-4d2d-9440-946c003025c0",
   "metadata": {},
   "source": [
    "# Example Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2e3a110-538a-4bed-9cb7-5863a3e12844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "# Example Client Messages\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "cm = ClientMessage(cbsp_pb2)\n",
    "\n",
    "d = cm.serializeGetParametersMsg({'a':'b'})\n",
    "d = cm.serializeSendResultsMsg({'a':'b'},{'c':'d'})\n",
    "d = cm.serializeGetConfigMsg({'e':'f'})\n",
    "d = cm.serializeSendParametersMsg({'a':'b'}, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5174c8fb-21c9-4312-91d3-e29ea36a1ed3",
   "metadata": {},
   "source": [
    "# Testing Parameter Sending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b36e346-476f-464a-95cb-6675dc3700a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Define image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load and preprocess the test image\n",
    "image_locn = \"./wombat.jpg\"\n",
    "image = Image.open(image_locn).convert(\"RGB\")\n",
    "input_image = transform(image).unsqueeze(0)\n",
    "\n",
    "# Load labels for ImageNet classes\n",
    "LABELS_URL = \"https://github.com/anishathalye/imagenet-simple-labels/blob/master/imagenet-simple-labels.json\"\n",
    "labels = requests.get(LABELS_URL).json()['payload']['blob']['rawLines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68061a43-8f16-41d6-8b66-68d5b1785e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet-18 model\n",
    "torch.manual_seed(420)\n",
    "model = models.resnet18(pretrained=False)\n",
    "model.eval()\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaa74118-665e-475f-ab65-9c2340ed85af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Alaskan tundra wolf\",: 0.64%\n",
      "\"swimsuit\",: 0.51%\n",
      "\"fire salamander\",: 0.50%\n",
      "\"strawberry\",: 0.45%\n",
      "\"ski\",: 0.45%\n"
     ]
    }
   ],
   "source": [
    "# Get the top 5 predictions\n",
    "_, indices = torch.topk(output, 5)\n",
    "probs = torch.nn.functional.softmax(output, dim=1)[0] * 100\n",
    "\n",
    "# Print the top 5 classes and their probabilities\n",
    "for i in range(5):\n",
    "    print(f\"{labels[indices[0][i]]}: {probs[indices[0][i]].item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "658fbfba-6672-4483-9db6-854d3fd790e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelc = model\n",
    "d = cm.serializeSendParametersMsg({'seed':'69'}, modelc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff161462-9ee0-44d0-987d-f430fa1a25b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recv_param_test(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_image)\n",
    "    # Get the top 5 predictions\n",
    "    _, indices = torch.topk(output, 5)\n",
    "    probs = torch.nn.functional.softmax(output, dim=1)[0] * 100\n",
    "    \n",
    "    # Print the top 5 classes and their probabilities\n",
    "    for i in range(5):\n",
    "        print(f\"{labels[indices[0][i]]}: {probs[indices[0][i]].item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68770ac4-0039-4f13-8442-a705171ef11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received Server's SendParameters message\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aa': 'moddale'} 0 ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n",
      "\"lipstick\",: 1.37%\n",
      "\"jack-o'-lantern\",: 0.79%\n",
      "\"cabbage\",: 0.77%\n",
      "\"yurt\",: 0.64%\n",
      "\"bookcase\",: 0.62%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "# Convert parameters to NumPy arrays\n",
    "params = [val.cpu().numpy() for _, val in model.state_dict().items()]\n",
    "params_grpc = []\n",
    "for i in params:\n",
    "    params_grpc.append(cbsp_pb2.ParameterBytes(tensor=i.tobytes(), shape=list(np.shape(i))))\n",
    "    break\n",
    "\n",
    "def run_client():\n",
    "    options = [\n",
    "        ('grpc.max_receive_message_length', 1024 * 1024 * 1000)  # Adjust the size as needed\n",
    "    ]\n",
    "    channel = grpc.insecure_channel('localhost:50052', options=options)  # Replace with the server address\n",
    "    stub = cbsp_pb2_grpc.CommunicationServiceStub(channel)\n",
    "\n",
    "    # Create a client message (for example, SendResults)\n",
    "    # client_messag1 = cbsp_pb2.ClientMessage(\n",
    "    #     send_parameters=cbsp_pb2.ClientMessage.SendParameters(\n",
    "    #         type=cbsp_pb2.ClientMessage.SEND_PARAMETERS,\n",
    "    #         info={'lol':cbsp_pb2.Constant(string='lol')},\n",
    "    #         parameters=cbsp_pb2.PytorchParameters(parameters=params_grpc, dtype=str(params[0].dtype))\n",
    "    #     )\n",
    "    # )\n",
    "    \n",
    "    sm = ServerMessage(cbsp_pb2)\n",
    "    # Send the client message and get the server response\n",
    "    response = stub.BidirectionalStream(d)\n",
    "    \n",
    "    match response.WhichOneof(\"server_message\"):\n",
    "            case \"get_parameters\":\n",
    "                print(\"Received Server's SendParameters message\")\n",
    "                model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "                info, params, model = sm.deserializeSendParametersMsg(response, model)\n",
    "                # info, param_float, param_dtype, model = cm.deserializeSendParametersMsg(request_iterator, model) # Alternate way to get updated model as well\n",
    "                print(info, len(params), model)\n",
    "                recv_param_test(model)\n",
    "            case \"send_config\":\n",
    "                print(\"Received SendConfig message\")\n",
    "                info = sm.deserializeSendConfigMsg(response)\n",
    "                print(info)\n",
    "            case \"normal_response\":\n",
    "                print(\"Received NormalResponse message\")\n",
    "                info, response = sm.deserializeNormalResponseMsg(response)\n",
    "                print(info, response)\n",
    "            case _:\n",
    "                print(\"ERROR: Received unknown message type\")\n",
    "        \n",
    "    # # Iterate through the response\n",
    "    # for server_message in response:\n",
    "    #     if server_message.HasField('normal_response'):\n",
    "    #         print(\"Server response:\", server_message.normal_response.response)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f98c7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
