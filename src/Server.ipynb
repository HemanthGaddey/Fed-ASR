{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8433fe41-d98e-4617-9fb9-c6afeaafe5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import grpc\n",
    "import proto.cbsp_pb2 as cbsp_pb2\n",
    "import proto.cbsp_pb2_grpc as cbsp_pb2_grpc\n",
    "from concurrent import futures\n",
    "import pickle\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "445b5051-1df3-460d-91a2-715e9304a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClientMessage():\n",
    "    def __init__(self, CbspMsg):\n",
    "        self.CbspMsg = CbspMsg\n",
    "    \n",
    "    # Note: All the serialization and deserialization use Lazy Dict serialization!\n",
    "    # Utility Functions\n",
    "    def torchModel2NumpyParams(self, model): \n",
    "        params = [val.cpu().numpy() for _, val in model.state_dict().items()]\n",
    "        return params\n",
    "\n",
    "    def numpyParams2TorchModel(self, model, params): \n",
    "        model_params = model.state_dict()\n",
    "        for key, val in model_params.items():\n",
    "            if len(params) > 0:\n",
    "                model_params[key] = torch.tensor(params.pop(0))\n",
    "        model.load_state_dict(model_params)\n",
    "        return model\n",
    "                        \n",
    "    # Orphaned Function\n",
    "    def serializeDictStrict(self, info): \n",
    "        info_ = {}\n",
    "        for key, value in info.items():\n",
    "            assert (isinstance(key, str)), \"Dictionary Keys contain non String values!\"\n",
    "            assert (isinstance(value, int) or isinstance(value, str) or isinstance(value, float) or isinstance(value, bool)), \"Dictionary Contains Unsupported Value types!\"\n",
    "            \n",
    "            value_ = None\n",
    "            if(isinstance(value, bool)):    \n",
    "                value_ = self.CbspMsg.Constant(bool=value)\n",
    "            elif(isinstance(value, int)):    \n",
    "                value_ = self.CbspMsg.Constant(sint64=value)\n",
    "            elif(isinstance(value, float)):    \n",
    "                value_ = self.CbspMsg.Constant(double=value)\n",
    "            elif(isinstance(value, str)):    \n",
    "                value_ = self.CbspMsg.Constant(string=value)\n",
    "            else:\n",
    "                print(\"UNKNOWN ERROR CONVERTING DICT TO GRPC MSG FORMAT :( (chusko)\")\n",
    "                \n",
    "            info_[key] = value_\n",
    "        return info_\n",
    "    \n",
    "    def serializeDictLazy(self, info):\n",
    "        return {\"dict\":self.CbspMsg.Constant(string=str(info))}\n",
    "        \n",
    "    def deserializeDictLazy(self, info):\n",
    "        return eval(info['dict'].string)\n",
    "\n",
    "    # Msg Serialization Functions\n",
    "    def serializePytorchParams(self, params): \n",
    "        params_bytelist = []\n",
    "        for i in params:\n",
    "            params_bytelist.append(cbsp_pb2.ParameterBytes(tensor=pickle.dumps(i), shape=[0]))\n",
    "        return cbsp_pb2.PytorchParameters(parameters=params_bytelist, dtype=str(params[0].dtype))\n",
    "                      \n",
    "    def serializeGetParametersMsg(self, info):\n",
    "        info_=self.serializeDictLazy(info)\n",
    "        return self.CbspMsg.ClientMessage(\n",
    "            get_parameters = self.CbspMsg.ClientMessage.GetParameters(\n",
    "                type=self.CbspMsg.ClientMessage.GET_PARAMETERS,\n",
    "                info=info_\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def serializeGetConfigMsg(self, info):\n",
    "        info_=self.serializeDictLazy(info)\n",
    "        return self.CbspMsg.ClientMessage(\n",
    "            get_config = self.CbspMsg.ClientMessage.GetConfig(\n",
    "                type=self.CbspMsg.ClientMessage.GET_CONFIG,\n",
    "                info=info_\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def serializeSendParametersMsg(self, info, model):\n",
    "        info_=self.serializeDictLazy(info)\n",
    "        params = self.torchModel2NumpyParams(model)\n",
    "        params_grpc=self.serializePytorchParams(params) # params_grpc\n",
    "        return self.CbspMsg.ClientMessage(\n",
    "            send_parameters = self.CbspMsg.ClientMessage.SendParameters(\n",
    "                type=self.CbspMsg.ClientMessage.SEND_PARAMETERS,\n",
    "                info=info_,\n",
    "                parameters=params_grpc\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def serializeSendResultsMsg(self, info, results):\n",
    "        info_=self.serializeDictLazy(info)\n",
    "        results_=self.serializeDictLazy(results)\n",
    "        return self.CbspMsg.ClientMessage(\n",
    "            send_results = self.CbspMsg.ClientMessage.SendResults(\n",
    "                type=self.CbspMsg.ClientMessage.SEND_RESULTS,\n",
    "                info=info_,\n",
    "                results=results_\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Msg Deserialization Functions\n",
    "    def deserializePytorchParams(self, params_grpc): \n",
    "        params_bytelist = params_grpc.parameters\n",
    "        params_dtype = params_grpc.dtype # Redundant\n",
    "        params = []\n",
    "        for i in params_bytelist:\n",
    "            params.append(pickle.loads(i.tensor))\n",
    "        return params\n",
    "        \n",
    "    def deserializeGetParametersMsg(self, request):\n",
    "        info_ = request.get_parameters.info\n",
    "\n",
    "        info = self.deserializeDictLazy(info_)\n",
    "\n",
    "        return info\n",
    "        \n",
    "    def deserializeGetConfigMsg(self, request):\n",
    "        info_ = request.get_config.info\n",
    "\n",
    "        info = self.deserializeDictLazy(info_)\n",
    "\n",
    "        return info\n",
    "        \n",
    "    def deserializeSendParametersMsg(self, request, model=None):\n",
    "        info_ = request.send_parameters.info\n",
    "        params_grpc = request.send_parameters.parameters\n",
    "        \n",
    "        info = self.deserializeDictLazy(info_)\n",
    "        params = self.deserializePytorchParams(params_grpc)\n",
    "\n",
    "        if(model):\n",
    "            model = self.numpyParams2TorchModel(model, params)\n",
    "\n",
    "        return info, params, model # dtype process is sus\n",
    "\n",
    "    def deserializeSendResultsMsg(self, request):\n",
    "        info_ = request.send_results.info\n",
    "        results_ = request.send_results.results\n",
    "\n",
    "        info = self.deserializeDictLazy(info_)\n",
    "        results = self.deserializeDictLazy(results_)\n",
    "\n",
    "        return info, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc32b86f-b164-499a-bda9-27278b871700",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ServerMessage():\n",
    "    def __init__(self, CbspMsg):\n",
    "        self.CbspMsg = CbspMsg\n",
    "        \n",
    "    # TODO: Reduce redundancy by separating/modularizing utility functions code\n",
    "    # Note: All the serialization and deserialization use Lazy Dict serialization!\n",
    "    # Utility Functions\n",
    "    def torchModel2NumpyParams(self, model): \n",
    "        params = [val.cpu().numpy() for _, val in model.state_dict().items()]\n",
    "        return params\n",
    "\n",
    "    def numpyParams2TorchModel(self, model, params): \n",
    "        model_params = model.state_dict()\n",
    "        for key, val in model_params.items():\n",
    "            if len(params) > 0:\n",
    "                model_params[key] = torch.tensor(params.pop(0))\n",
    "        model.load_state_dict(model_params)\n",
    "        return model\n",
    "        \n",
    "    # Orphaned Function\n",
    "    def serializeDictStrict(self, info): \n",
    "        info_ = {}\n",
    "        for key, value in info.items():\n",
    "            assert (isinstance(key, str)), \"Dictionary Keys contain non String values!\"\n",
    "            assert (isinstance(value, int) or isinstance(value, str) or isinstance(value, float) or isinstance(value, bool)), \"Dictionary Contains Unsupported Value types!\"\n",
    "            \n",
    "            value_ = None\n",
    "            if(isinstance(value, bool)):    \n",
    "                value_ = self.CbspMsg.Constant(bool=value)\n",
    "            elif(isinstance(value, int)):    \n",
    "                value_ = self.CbspMsg.Constant(sint64=value)\n",
    "            elif(isinstance(value, float)):    \n",
    "                value_ = self.CbspMsg.Constant(double=value)\n",
    "            elif(isinstance(value, str)):    \n",
    "                value_ = self.CbspMsg.Constant(string=value)\n",
    "            else:\n",
    "                print(\"UNKNOWN ERROR CONVERTING DICT TO GRPC MSG FORMAT :( (chusko)\")\n",
    "                \n",
    "            info_[key] = value_\n",
    "        return info_\n",
    "    \n",
    "    def serializeDictLazy(self, info):\n",
    "        return {\"dict\":self.CbspMsg.Constant(string=str(info))}\n",
    "        \n",
    "    def deserializeDictLazy(self, info):\n",
    "        return eval(info['dict'].string)\n",
    "\n",
    "    # Msg Serialization Functions        \n",
    "    def serializePytorchParams(self, params): \n",
    "        params_bytelist = []\n",
    "        for i in params:\n",
    "            params_bytelist.append(cbsp_pb2.ParameterBytes(tensor=pickle.dumps(i), shape=[0]))\n",
    "        return cbsp_pb2.PytorchParameters(parameters=params_bytelist, dtype=str(params[0].dtype))\n",
    "   \n",
    "    def serializeSendParametersMsg(self, info, model): # TODO: Get Better Naming for these since pytorch specific\n",
    "        info_=self.serializeDictLazy(info)\n",
    "        params = self.torchModel2NumpyParams(model)\n",
    "        params_grpc=self.serializePytorchParams(params) # params_grpc\n",
    "        return self.CbspMsg.ServerMessage(\n",
    "            get_parameters = self.CbspMsg.ServerMessage.SendParameters( # TODO: Change this get_parameters to send_parameters\n",
    "                type=self.CbspMsg.ServerMessage.SEND_PARAMETERS,\n",
    "                info=info_,\n",
    "                parameters=params_grpc\n",
    "            )\n",
    "        )       \n",
    "\n",
    "    def serializeSendConfigMsg(self, info):\n",
    "        info_=self.serializeDictLazy(info)\n",
    "        return self.CbspMsg.ServerMessage(\n",
    "            send_config = self.CbspMsg.ServerMessage.SendConfig(\n",
    "                type=self.CbspMsg.ServerMessage.SEND_CONFIG,\n",
    "                info=info_\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def serializeNormalResponseMsg(self, info, response):\n",
    "        info_=self.serializeDictLazy(info)\n",
    "        return self.CbspMsg.ServerMessage(\n",
    "            normal_response = self.CbspMsg.ServerMessage.NormalResponse(\n",
    "                type=self.CbspMsg.ServerMessage.MESSAGE_TYPE.NORMAL_RESPONSE,\n",
    "                info=info_,\n",
    "                response=response\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    # Msg Deserialization Functions\n",
    "    def deserializePytorchParams(self, params_grpc): \n",
    "        params_bytelist = params_grpc.parameters\n",
    "        params_dtype = params_grpc.dtype # Redundant\n",
    "        params = []\n",
    "        for i in params_bytelist:\n",
    "            params.append(pickle.loads(i.tensor))\n",
    "        return params\n",
    "        \n",
    "    def deserializeSendParametersMsg(self, request, model=None):\n",
    "        info_ = request.get_parameters.info # TODO: Change this get_parameters to send_parameters\n",
    "        param_bytes = request.get_parameters.parameters # TODO: Change this get_parameters to send_parameters\n",
    "        \n",
    "        info = self.deserializeDictLazy(info_)\n",
    "        params = self.deserializePytorchParams(param_bytes)\n",
    "\n",
    "        if(model):\n",
    "            model = self.numpyParams2TorchModel(model, params)\n",
    "\n",
    "        return info, params, model # dtype process is sus\n",
    "\n",
    "    def deserializeSendConfigMsg(self, request):\n",
    "        info_ = request.send_config.info\n",
    "        info = self.deserializeDictLazy(info_)\n",
    "        return info\n",
    "        \n",
    "    def deserializeNormalResponseMsg(self, request):\n",
    "        info_ = request.normal_response.info\n",
    "        info = self.deserializeDictLazy(info_)\n",
    "        response = request.normal_response.response\n",
    "        return info, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db01458b-f94b-4aec-a4fc-5058a6cad6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4c5a4b-5538-4017-9aee-f3e217563aa5",
   "metadata": {},
   "source": [
    "# Testing Parameter Receiving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb111f5b-e4b5-4d44-9f48-1100ea95c769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Define image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load and preprocess the test image\n",
    "image_locn = \"./wombat.jpg\"\n",
    "image = Image.open(image_locn).convert(\"RGB\")\n",
    "input_image = transform(image).unsqueeze(0)\n",
    "\n",
    "# Load labels for ImageNet classes\n",
    "LABELS_URL = \"https://github.com/anishathalye/imagenet-simple-labels/blob/master/imagenet-simple-labels.json\"\n",
    "labels = requests.get(LABELS_URL).json()['payload']['blob']['rawLines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "451a7246-70c5-4cbf-851e-ec4bb2e19c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet-18 model\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c47fff5a-668e-4869-9c69-9395f1912b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"wombat\",: 97.59%\n",
      "\"beaver\",: 2.00%\n",
      "\"marmot\",: 0.26%\n",
      "\"cottontail rabbit\",: 0.04%\n",
      "\"otter\",: 0.02%\n"
     ]
    }
   ],
   "source": [
    "# Get the top 5 predictions\n",
    "_, indices = torch.topk(output, 5)\n",
    "probs = torch.nn.functional.softmax(output, dim=1)[0] * 100\n",
    "\n",
    "# Print the top 5 classes and their probabilities\n",
    "for i in range(5):\n",
    "    print(f\"{labels[indices[0][i]]}: {probs[indices[0][i]].item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "277aa0ee-e9b6-475b-8c4e-e7154cee7a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recv_param_test(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_image)\n",
    "    # Get the top 5 predictions\n",
    "    _, indices = torch.topk(output, 5)\n",
    "    probs = torch.nn.functional.softmax(output, dim=1)[0] * 100\n",
    "    \n",
    "    # Print the top 5 classes and their probabilities\n",
    "    for i in range(5):\n",
    "        print(f\"{labels[indices[0][i]]}: {probs[indices[0][i]].item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ff5fa23-f07e-4316-b2c0-d1a43448912f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server Started :D\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received SendParameters message\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': '69'} 0 ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n",
      "\"Alaskan tundra wolf\",: 0.64%\n",
      "\"swimsuit\",: 0.51%\n",
      "\"fire salamander\",: 0.50%\n",
      "\"strawberry\",: 0.45%\n",
      "\"ski\",: 0.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import grpc\n",
    "import proto.cbsp_pb2 as cbsp_pb2\n",
    "import proto.cbsp_pb2_grpc as cbsp_pb2_grpc\n",
    "from concurrent import futures\n",
    "\n",
    "class CommunicationService(cbsp_pb2_grpc.CommunicationServiceServicer):\n",
    "    # As per docs, this class is not supposed to have init function, I added this just so I can add the cbsp_pb2\n",
    "    # as class' internal variable and also ClientManager as internal object, consider removing if unexplained errors persist \n",
    "    def __init__(self, cbsp_pb2):\n",
    "        self.CbspMsg = cbsp_pb2\n",
    "        self.cm = ClientMessage(self.CbspMsg)\n",
    "        self.sm = ServerMessage(self.CbspMsg)\n",
    "        \n",
    "    def BidirectionalStream(self, request_iterator, context):        \n",
    "        # print(hasattr(request_iterator,'send_results')) # Warning: Doesn't work - A grpc msg has attributes for all msg types, just that rest of them are empty\n",
    "        # Check the type of message received and process accordingly\n",
    "        if(request_iterator.WhichOneof(\"client_message\") == \"get_parameters\"):\n",
    "            print(\"Received GetParameters message\")\n",
    "            info = self.cm.deserializeGetParametersMsg(request_iterator)\n",
    "            print(info)\n",
    "        elif(request_iterator.WhichOneof(\"client_message\") == \"get_config\"):\n",
    "            print(\"Received GetConfig message\")\n",
    "            info = self.cm.deserializeGetConfigMsg(request_iterator)\n",
    "            print(info)\n",
    "        elif(request_iterator.WhichOneof(\"client_message\") == \"send_parameters\"):\n",
    "            print(\"Received SendParameters message\")\n",
    "            model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "            info, params, model = self.cm.deserializeSendParametersMsg(request_iterator, model)\n",
    "            print(info,len(params),model)\n",
    "            recv_param_test(model)\n",
    "            # info, param_float, param_dtype, model = cm.deserializeSendParametersMsg(request_iterator, model) # Alternate way to get updated model as well\n",
    "        elif(request_iterator.WhichOneof(\"client_message\") == \"send_results\"):\n",
    "            print(\"Received SendResults message\")\n",
    "            info, results = self.cm.deserializeSendResultsMsg(request_iterator)\n",
    "            print(info, results)\n",
    "        else:\n",
    "            print(\"ERROR: Received unknown message type\")\n",
    "        model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\n",
    "        server_response = self.sm.serializeSendParametersMsg({'aa':'moddale'}, model)\n",
    "        return server_response\n",
    "\n",
    "def run_server():\n",
    "    options = [\n",
    "        ('grpc.max_receive_message_length', 1024 * 1024 * 1000)  # Adjust the size as needed\n",
    "    ]\n",
    "    server = grpc.server(futures.ThreadPoolExecutor(), options=options)\n",
    "    cbsp_pb2_grpc.add_CommunicationServiceServicer_to_server(CommunicationService(cbsp_pb2), server)\n",
    "    \n",
    "    server.add_insecure_port('[::]:50052')\n",
    "    server.start()\n",
    "    print('Server Started...')\n",
    "    server.wait_for_termination()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ebc19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
